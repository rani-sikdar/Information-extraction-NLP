{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'authorities', 'fined', 'Google', 'a', 'record', '$', '5.1', 'billion', 'on', 'Wednesday', 'for', 'abusing', 'its', 'power', 'in', 'the', 'mobile', 'phone', 'market', 'and', 'ordered', 'the', 'company', 'to', 'alter', 'its', 'practices']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text = \"\"\"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\"\"\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "# doc = nlp(text)\n",
    "doc = nlp.tokenizer(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\"\"\n",
    "# create list of sentence tokens\n",
    "d=nlp(text)\n",
    "sents_list = []\n",
    "for sent in d.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['amount', 'who', 'just', 'amongst', 'by', 'though', 'twelve', 'side', 'beyond', 'above', 'bottom', 'itself', 'seems', 'other', 'up', 'another', 'whereupon', 'few', 'now', 'last']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [European, authorities, fined, Google, record, $, 5.1, billion, Wednesday, abusing, power, mobile, phone, market, ordered, company, alter, practices]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Implementation of stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "# filtering stop words\n",
    "for word in d:\n",
    "    if word.is_stop==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>European</td>\n",
       "      <td>European</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>authorities</td>\n",
       "      <td>authorities</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fined</td>\n",
       "      <td>fined</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google</td>\n",
       "      <td>Google</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>x</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>record</td>\n",
       "      <td>record</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>d.d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>billion</td>\n",
       "      <td>billion</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>abusing</td>\n",
       "      <td>abusing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>its</td>\n",
       "      <td>its</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>power</td>\n",
       "      <td>power</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mobile</td>\n",
       "      <td>mobile</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phone</td>\n",
       "      <td>phone</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>market</td>\n",
       "      <td>market</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ordered</td>\n",
       "      <td>ordered</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>company</td>\n",
       "      <td>company</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>alter</td>\n",
       "      <td>alter</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>its</td>\n",
       "      <td>its</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>practices</td>\n",
       "      <td>practices</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1 2 3 4      5      6      7\n",
       "0      European     European        Xxxxx   True  False\n",
       "1   authorities  authorities         xxxx   True  False\n",
       "2         fined        fined         xxxx   True  False\n",
       "3        Google       Google        Xxxxx   True  False\n",
       "4             a            a            x   True   True\n",
       "5        record       record         xxxx   True  False\n",
       "6             $            $            $  False  False\n",
       "7           5.1          5.1          d.d  False  False\n",
       "8       billion      billion         xxxx   True  False\n",
       "9            on           on           xx   True   True\n",
       "10    Wednesday    Wednesday        Xxxxx   True  False\n",
       "11          for          for          xxx   True   True\n",
       "12      abusing      abusing         xxxx   True  False\n",
       "13          its          its          xxx   True   True\n",
       "14        power        power         xxxx   True  False\n",
       "15           in           in           xx   True   True\n",
       "16          the          the          xxx   True   True\n",
       "17       mobile       mobile         xxxx   True  False\n",
       "18        phone        phone         xxxx   True  False\n",
       "19       market       market         xxxx   True  False\n",
       "20          and          and          xxx   True   True\n",
       "21      ordered      ordered         xxxx   True  False\n",
       "22          the          the          xxx   True   True\n",
       "23      company      company         xxxx   True  False\n",
       "24           to           to           xx   True   True\n",
       "25        alter        alter         xxxx   True  False\n",
       "26          its          its          xxx   True   True\n",
       "27    practices    practices         xxxx   True  False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part-of-speech taging \n",
    "nlp = en_core_web_sm.load()\n",
    "data = []\n",
    "for token in d:\n",
    "    data.append([token.text,token.lemma_ ,token.pos_ ,token.tag_ ,token.dep_,\n",
    "         token.shape_, token.is_alpha, token.is_stop])\n",
    "    #text | lemma | pos | tag | dep | shape | alpha | stop \n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the dependency parse\n",
    "nlp = en_core_web_sm.load()\n",
    "displacy.render(nlp(doc))   #syntactic dependies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:\nhttps://spacy.io/usage/models",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a6905ff0cc1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[1;31m#noun chunks - \"based noun phrases -flat phrases that have a noun as their head\"\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     data_.append([chunk.text,chunk.root.text, chunk.root.dep_,\n\u001b[0;32m      6\u001b[0m          chunk.root.head.text])    # text | root text | root dep | root head text \n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mnoun_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:\nhttps://spacy.io/usage/models"
     ]
    }
   ],
   "source": [
    "#dependency parsing \n",
    "        #noun chunks - \"based noun phrases -flat phrases that have a noun as their head\"\\\n",
    "data_ = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    data_.append([chunk.text,chunk.root.text, chunk.root.dep_,\n",
    "         chunk.root.head.text])    # text | root text | root dep | root head text \n",
    "\n",
    "pd.DataFrame(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Navigating the parse tree\n",
    "\n",
    "#head  #child  #dep\n",
    "data__ =[]\n",
    "for token in doc :\n",
    "    data__.append([token.text,token.dep_,token.head.text,token.head.pos,\n",
    "                  [child for child in token.children]])  #text | dep | head text | head pos | children\n",
    "\n",
    "pd.DataFrame(data__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nmaed entity recoginition \n",
    "_data_ =[]\n",
    "for ent in doc.ents:\n",
    "    _data_.append([ent.text,ent.start_char,ent.end_char,ent.label_])\n",
    "\n",
    "pd.DataFrame(_data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing named entity\n",
    "pprint([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    "displacy.render(doc,style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token\n",
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting named entity from an article\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "text_file =open(\"corpus.txt\",\"w\",encoding=\"utf-8\")\n",
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "        \n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "\n",
    "ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
    "text_file.write(ny_bb)\n",
    "article = nlp(ny_bb)\n",
    "len(article.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x.label_ for x in article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [x.text for x in article.ents]\n",
    "Counter(items).most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x for x in article.sents]\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(sentences[10])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(sentences[10])), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                      for y\n",
    "                                      in nlp(str(sentences[10])) \n",
    "                                      if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict([(str(x), x.label_) for x in nlp(str(sentences[10])).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "mango = nlp(u'europe')\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(x, x.ent_iob_, x.ent_type_) for x in sentences[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displacy.render(article,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash_dangerously_set_inner_html as insert_html\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import spacy\n",
    "from dash.dependencies import Input, Output\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "file = open('corpus.txt',encoding=\"utf-8\").read()\n",
    "doc = nlp(file)\n",
    "\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "\n",
    "def Calculate(doc):\n",
    "    #text | lemma | pos | tag | dep | shape | alpha | stop\n",
    "    speech_part_tag = [[token.text, token.lemma_, token.pos_, token.tag_,\n",
    "                        token.dep_, token.shape_, token.is_alpha, token.is_stop] for token in doc]\n",
    "    nouns = [[chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "              chunk.root.head.text] for chunk in doc.noun_chunks]\n",
    "    # text | root text | root dep | root head text\n",
    "    _entity_ = [[ent.text, ent.start_char, ent.end_char, ent.label_]\n",
    "                for ent in doc.ents]\n",
    "    html = displacy.render(doc, style='ent', jupyter=False)\n",
    "    html1 = displacy.render(doc,style='dep', jupyter=False)\n",
    "    return html, html1, _entity_, nouns, speech_part_tag\n",
    "\n",
    "\n",
    "def generate_table(dataframe, max_rows=20):\n",
    "    return html.Table(\n",
    "        # Header\n",
    "        [html.Tr([html.Th(col) for col in dataframe.columns])] +\n",
    "\n",
    "        # Body\n",
    "        [html.Tr([\n",
    "            html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "        ]) for i in range(min(len(dataframe), max_rows))]\n",
    "    )\n",
    "\n",
    "\n",
    "Html, Html1, _entity_, nouns, speech_part_tag = Calculate(doc)\n",
    "\n",
    "\n",
    "app.layout = html.Div(id=\"display\", children=[\n",
    "    html.H1(children='Feature Extraction'),\n",
    "    html.Div(children=file),\n",
    "\n",
    "    dcc.Tabs(id=\"tabs\", value='tab-1', children=[\n",
    "        dcc.Tab(label='Tab one', value='tab-1'),\n",
    "        dcc.Tab(label='Tab two', value='tab-2'),\n",
    "        dcc.Tab(label='Tab three', value='tab-3'),\n",
    "        dcc.Tab(label='Color Represntion', value='tab-4'),\n",
    "    ]),\n",
    "    html.Div(id='tabs-content')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(Output('tabs-content', 'children'),\n",
    "              [Input('tabs', 'value')])\n",
    "def render_content(tab):\n",
    "    if tab == 'tab-1':\n",
    "        return generate_table(pd.DataFrame(speech_part_tag, columns='text | lemma | pos | tag | dep | shape | alpha | stop '.split(\" | \")))\n",
    "    elif tab == 'tab-2':\n",
    "        return html.Div([generate_table(pd.DataFrame(_entity_))])\n",
    "    elif tab == 'tab-3':\n",
    "        return html.Div([generate_table(pd.DataFrame(nouns, columns=' text | root text | root dep | root head text'.split(\" | \")))])\n",
    "    elif tab == 'tab-4':\n",
    "        return html.Div(insert_html.DangerouslySetInnerHTML(Html))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, use_reloader=False)\n",
    "    #app.run(debug=True, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
